{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28abd92-9f85-4f58-977e-3cbb33dfe26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f0f43f-0664-4dea-bac7-e2223a6f2f49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv==2.1.0) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv==2.1.0) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10->arxiv==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: openai==1.3.4 in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.4) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (1.10.13)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.4) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.4) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.4) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.4) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.4) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.4) (0.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install arxiv==2.1.0\n",
    "!pip3 install python-dotenv tiktoken\n",
    "# !pip install openai==0.27.8\n",
    "# !pip install openai==1.2.3\n",
    "!pip install openai==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73f2f51-2af8-4924-939c-e5f08c62f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import arxiv\n",
    "import openai\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b052b9-0864-4791-b144-bbcce9933abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891e7219-d84f-49d3-b928-3b54687ef89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612b548f-a137-4909-a4eb-6028335e0635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e30177-9973-42f1-88ef-ddb51edff296",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c94d88c-cace-48eb-ac0e-3b69e7625069",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "### 指示 ###\n",
    "論文の内容を理解した上で，重要なポイントを箇条書きで3点書いてください。\n",
    "\n",
    "### 箇条書きの制約 ###\n",
    "- 最大3個\n",
    "- 日本語\n",
    "- 箇条書き1個を50文字以内\n",
    "\n",
    "### 対象とする論文の内容 ###\n",
    "{text}\n",
    "\n",
    "### 出力形式 ###\n",
    "タイトル(和名)\n",
    "\n",
    "- 箇条書き1\n",
    "- 箇条書き2\n",
    "- 箇条書き3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca866b4-777e-4e42-aa01-ae23811639de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arXivの更新頻度を加味して、5日前の論文を検索\n",
    "N_DAYS = 5\n",
    "\n",
    "MAX_RESULT = 24  # 取得する論文数の上限\n",
    "# MODEL_NAME = \"gpt-3.5-turbo-0613\"\n",
    "MODEL_NAME = \"gpt-3.5-turbo-1106\"\n",
    "# MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "TEMPERATURE = 0.8\n",
    "# OpenAIクライアントの初期化\n",
    "client = OpenAI()\n",
    "\n",
    "# テンプレートを用意\n",
    "QUERY_TEMPLATE = '%28 ti:%22{}%22 OR abs:%22{}%22 %29 AND submittedDate: [{} TO {}]'\n",
    "\n",
    "# 検索を行い、結果を取得する関数\n",
    "def search_arxiv(keyword):\n",
    "    # Construct the default API client.\n",
    "    client = arxiv.Client()\n",
    "    # 2日前からN_DAYS前までの論文を検索\n",
    "    today = dt.datetime.today() - dt.timedelta(days=2)\n",
    "    # today = dt.datetime.today()\n",
    "    \n",
    "    base_date = today - dt.timedelta(days=N_DAYS)\n",
    "    query = QUERY_TEMPLATE.format(keyword, keyword, base_date.strftime(\"%Y%m%d%H%M%S\"), today.strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=MAX_RESULT,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "\n",
    "    # results = []\n",
    "    # for result in search.results():\n",
    "    #     # カテゴリーチェック\n",
    "    #     # if not set(result.categories) & CATEGORIES:\n",
    "    #     #     continue\n",
    "    #     # 要約内でのキーワードの存在チェック\n",
    "    #     if keyword.lower() in result.summary.lower():\n",
    "    #         results.append(result)\n",
    "    results = client.results(search)\n",
    "    return results\n",
    "\n",
    "# 論文の要約を取得する関数\n",
    "def get_summary(result):\n",
    "    text = f\"title: {result.title}\\nbody: {result.summary}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    \n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=MODEL_NAME,\n",
    "    #     messages=messages,\n",
    "    #     temperature=TEMPERATURE,\n",
    "    # )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    # return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88fcab98-4988-4ae9-bff5-16fd4e90cb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: K-PERM: Personalized Response Generation Using Dynamic Knowledge Retrieval and Persona-Adaptive Queries\n",
      "published: 2023-12-29 18:59:58+00:00\n",
      "abstruct: Personalizing conversational agents can enhance the quality of conversations\n",
      "and increase user engagement. However, they often lack external knowledge to\n",
      "appropriately tend to a user's persona. This is particularly crucial for\n",
      "practical applications like mental health support, nutrition planning,\n",
      "culturally sensitive conversations, or reducing toxic behavior in\n",
      "conversational agents. To enhance the relevance and comprehensiveness of\n",
      "personalized responses, we propose using a two-step approach that involves (1)\n",
      "selectively integrating user personas and (2) contextualizing the response with\n",
      "supplementing information from a background knowledge source. We develop K-PERM\n",
      "(Knowledge-guided PErsonalization with Reward Modulation), a dynamic\n",
      "conversational agent that combines these elements. K-PERM achieves\n",
      "state-of-the-art performance on the popular FoCus dataset, containing\n",
      "real-world personalized conversations concerning global landmarks. We show that\n",
      "using responses from K-PERM can improve performance in state-of-the-art LLMs\n",
      "(GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing\n",
      "chatbots.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17748v1\n",
      "summary: K-PERM: パーソナライズされた応答生成におけるダイナミックな知識検索とペルソナ適応型クエリの使用\n",
      "\n",
      "- 対話エージェントのパーソナライズ化は会話の質を向上させ、ユーザーの関与を増加させる。\n",
      "- K-PERMは、ユーザーのペルソナを選択的に統合し、背景知識源から情報を補完して応答を文脈化する2段階のアプローチを提案する。\n",
      "- K-PERMはFoCusデータセットで最先端の性能を達成し、LLM（GPT 3.5）のパフォーマンスを10.5％向上させることが示された。\n",
      "--------------------------------------------------\n",
      "title: Jatmo: Prompt Injection Defense by Task-Specific Finetuning\n",
      "published: 2023-12-29 16:37:53+00:00\n",
      "abstruct: Large Language Models (LLMs) are attracting significant research attention\n",
      "due to their instruction-following abilities, allowing users and developers to\n",
      "leverage LLMs for a variety of tasks. However, LLMs are vulnerable to\n",
      "prompt-injection attacks: a class of attacks that hijack the model's\n",
      "instruction-following abilities, changing responses to prompts to undesired,\n",
      "possibly malicious ones. In this work, we introduce Jatmo, a method for\n",
      "generating task-specific models resilient to prompt-injection attacks. Jatmo\n",
      "leverages the fact that LLMs can only follow instructions once they have\n",
      "undergone instruction tuning. It harnesses a teacher instruction-tuned model to\n",
      "generate a task-specific dataset, which is then used to fine-tune a base model\n",
      "(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a\n",
      "dataset of inputs for the task: it uses the teacher model to generate outputs.\n",
      "For situations with no pre-existing datasets, Jatmo can use a single example,\n",
      "or in some cases none at all, to produce a fully synthetic dataset. Our\n",
      "experiments on six tasks show that Jatmo models provide the same quality of\n",
      "outputs on their specific task as standard LLMs, while being resilient to\n",
      "prompt injections. The best attacks succeeded in less than 0.5% of cases\n",
      "against our models, versus over 90% success rate against GPT-3.5-Turbo. We\n",
      "release Jatmo at https://github.com/wagner-group/prompt-injection-defense.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17673v1\n",
      "summary: Jatmo: Prompt Injection Defense by Task-Specific Finetuning\n",
      "\n",
      "- 大規模言語モデル（LLM）は、指示に従う能力から研究の注目を集めている。\n",
      "- Jatmoは、指示注入攻撃に対して強靭なタスク固有モデルを生成する手法。\n",
      "- Jatmoモデルは、特定のタスクにおいて通常のLLMと同等の出力品質を提供するが、指示注入に対して強靭である。\n",
      "--------------------------------------------------\n",
      "title: Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models\n",
      "published: 2023-12-29 15:57:49+00:00\n",
      "abstruct: The burgeoning interest in Multimodal Large Language Models (MLLMs), such as\n",
      "OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial\n",
      "realms. These models enhance Large Language Models (LLMs) with advanced visual\n",
      "understanding capabilities, facilitating their application in a variety of\n",
      "multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM\n",
      "designed specifically for multimodal integration. Despite its advancements,\n",
      "preliminary benchmarks indicate that Gemini lags behind GPT models in\n",
      "commonsense reasoning tasks. However, this assessment, based on a limited\n",
      "dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic\n",
      "commonsense reasoning potential. To address this gap, our study undertakes a\n",
      "thorough evaluation of Gemini's performance in complex reasoning tasks that\n",
      "necessitate the integration of commonsense knowledge across modalities. We\n",
      "carry out a comprehensive analysis of 12 commonsense reasoning datasets,\n",
      "ranging from general to domain-specific tasks. This includes 11 datasets\n",
      "focused solely on language, as well as one that incorporates multimodal\n",
      "elements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's\n",
      "competitive commonsense reasoning capabilities. Additionally, we identify\n",
      "common challenges faced by current LLMs and MLLMs in addressing commonsense\n",
      "problems, underscoring the need for further advancements in enhancing the\n",
      "commonsense reasoning abilities of these models.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17661v1\n",
      "summary: Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models\n",
      "\n",
      "- MLLMsは視覚理解能力を持つことで多様な多模式タスクに応用可能\n",
      "- GeminiはGPTモデルに比べて常識推論タスクで劣るという初期ベンチマーク結果\n",
      "- 実験によりGeminiの競争力ある常識推論能力を示し、LLMsおよびMLLMsが共通の課題に直面していることを明らかにした\n",
      "--------------------------------------------------\n",
      "title: Large Language Models for Generative Information Extraction: A Survey\n",
      "published: 2023-12-29 14:25:22+00:00\n",
      "abstruct: Information extraction (IE) aims to extract structural knowledge (such as\n",
      "entities, relations, and events) from plain natural language texts. Recently,\n",
      "generative Large Language Models (LLMs) have demonstrated remarkable\n",
      "capabilities in text understanding and generation, allowing for generalization\n",
      "across various domains and tasks. As a result, numerous works have been\n",
      "proposed to harness abilities of LLMs and offer viable solutions for IE tasks\n",
      "based on a generative paradigm. To conduct a comprehensive systematic review\n",
      "and exploration of LLM efforts for IE tasks, in this study, we survey the most\n",
      "recent advancements in this field. We first present an extensive overview by\n",
      "categorizing these works in terms of various IE subtasks and learning\n",
      "paradigms, then we empirically analyze the most advanced methods and discover\n",
      "the emerging trend of IE tasks with LLMs. Based on thorough review conducted,\n",
      "we identify several insights in technique and promising research directions\n",
      "that deserve further exploration in future studies. We maintain a public\n",
      "repository and consistently update related resources at:\n",
      "\\url{https://github.com/quqxui/Awesome-LLM4IE-Papers}.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17617v1\n",
      "summary: 大規模言語モデルによる生成情報抽出のための大規模言語モデル：調査\n",
      "\n",
      "- 情報抽出（IE）の目的は、自然言語テキストから構造化された知識（エンティティ、関係、イベントなど）を抽出すること。\n",
      "- 大規模言語モデル（LLM）は、テキスト理解と生成において顕著な能力を示し、様々な領域とタスクにおいて一般化が可能。\n",
      "- LLMを用いた情報抽出タスクに関する研究の包括的なシステマティック・レビューと探索を行い、最新の進展を調査。\n",
      "--------------------------------------------------\n",
      "title: The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey\n",
      "published: 2023-12-29 13:35:20+00:00\n",
      "abstruct: This scoping survey focuses on our current understanding of the design space\n",
      "for task-oriented LLM systems and elaborates on definitions and relationships\n",
      "among the available design parameters. The paper begins by defining a minimal\n",
      "task-oriented LLM system and exploring the design space of such systems through\n",
      "a thought experiment contemplating the performance of diverse LLM system\n",
      "configurations (involving single LLMs, single LLM-based agents, and multiple\n",
      "LLM-based agent systems) on a complex software development task and\n",
      "hypothesizes the results. We discuss a pattern in our results and formulate\n",
      "them into three conjectures. While these conjectures may be partly based on\n",
      "faulty assumptions, they provide a starting point for future research. The\n",
      "paper then surveys a select few design parameters: covering and organizing\n",
      "research in LLM augmentation, prompting techniques, and uncertainty estimation,\n",
      "and discussing their significance. The paper notes the lack of focus on\n",
      "computational and energy efficiency in evaluating research in these areas. Our\n",
      "survey findings provide a basis for developing the concept of linear and\n",
      "non-linear contexts, which we define and use to enable an agent-centric\n",
      "projection of prompting techniques providing a lens through which prompting\n",
      "techniques can be viewed as multi-agent systems. The paper discusses the\n",
      "implications of this lens, for the cross-pollination of research between LLM\n",
      "prompting and LLM-based multi-agent systems; and also, for the generation of\n",
      "synthetic training data based on existing prompting techniques in research. In\n",
      "all, the scoping survey presents seven conjectures that can help guide future\n",
      "research efforts.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17601v1\n",
      "summary: The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey\n",
      "\n",
      "- Minimal task-oriented LLM system and its design space are defined.\n",
      "- Three conjectures are formulated based on the survey results.\n",
      "- Lack of focus on computational and energy efficiency in LLM research is noted.\n",
      "--------------------------------------------------\n",
      "title: Action-Item-Driven Summarization of Long Meeting Transcripts\n",
      "published: 2023-12-29 12:33:21+00:00\n",
      "abstruct: The increased prevalence of online meetings has significantly enhanced the\n",
      "practicality of a model that can automatically generate the summary of a given\n",
      "meeting. This paper introduces a novel and effective approach to automate the\n",
      "generation of meeting summaries. Current approaches to this problem generate\n",
      "general and basic summaries, considering the meeting simply as a long dialogue.\n",
      "However, our novel algorithms can generate abstractive meeting summaries that\n",
      "are driven by the action items contained in the meeting transcript. This is\n",
      "done by recursively generating summaries and employing our action-item\n",
      "extraction algorithm for each section of the meeting in parallel. All of these\n",
      "sectional summaries are then combined and summarized together to create a\n",
      "coherent and action-item-driven summary. In addition, this paper introduces\n",
      "three novel methods for dividing up long transcripts into topic-based sections\n",
      "to improve the time efficiency of our algorithm, as well as to resolve the\n",
      "issue of large language models (LLMs) forgetting long-term dependencies. Our\n",
      "pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an\n",
      "approximately 4.98% increase from the current state-of-the-art result produced\n",
      "by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17581v1\n",
      "summary: 会議要約のアクションアイテム中心の自動生成\n",
      "\n",
      "- オンライン会議の増加により、会議要約の自動生成モデルの実用性が向上している。\n",
      "- 会議要約の新しい効果的なアプローチとして、アクションアイテムに基づいた要約生成アルゴリズムを導入。\n",
      "- 長い会議記録をトピックベースのセクションに分割し、効率的な要約アルゴリズムの時間効率を向上。\n",
      "--------------------------------------------------\n",
      "title: Building Efficient Universal Classifiers with Natural Language Inference\n",
      "published: 2023-12-29 10:18:36+00:00\n",
      "abstruct: Generative Large Language Models (LLMs) have become the mainstream choice for\n",
      "fewshot and zeroshot learning thanks to the universality of text generation.\n",
      "Many users, however, do not need the broad capabilities of generative LLMs when\n",
      "they only want to automate a classification task. Smaller BERT-like models can\n",
      "also learn universal tasks, which allow them to do any text classification task\n",
      "without requiring fine-tuning (zeroshot classification) or to learn new tasks\n",
      "with only a few examples (fewshot), while being significantly more efficient\n",
      "than generative LLMs. This paper (1) explains how Natural Language Inference\n",
      "(NLI) can be used as a universal classification task that follows similar\n",
      "principles as instruction fine-tuning of generative LLMs, (2) provides a\n",
      "step-by-step guide with reusable Jupyter notebooks for building a universal\n",
      "classifier, and (3) shares the resulting universal classifier that is trained\n",
      "on 33 datasets with 389 diverse classes. Parts of the code we share has been\n",
      "used to train our older zeroshot classifiers that have been downloaded more\n",
      "than 55 million times via the Hugging Face Hub as of December 2023. Our new\n",
      "classifier improves zeroshot performance by 9.4%.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17543v1\n",
      "summary: 効率的な汎用分類器の構築(NLI)\n",
      "\n",
      "- Generative Large Language Models (LLMs) が汎用性の高さからfewshotやzeroshot learningで主流になっている。\n",
      "- NLIを利用した汎用分類タスクの原則を解説し、再利用可能なJupyterノートブックを提供。\n",
      "- 33のデータセットで訓練された汎用分類器を共有し、zeroshotパフォーマンスを9.4%向上。\n",
      "--------------------------------------------------\n",
      "title: Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of LLMs\n",
      "published: 2023-12-29 09:33:35+00:00\n",
      "abstruct: CoT (Chain-of-Thought) is a way to solve reasoning problems for LLMs .\n",
      "Recently, many researches appear for improving the CoT capability of LLMs. In\n",
      "this work, we also proposed Olapa-MCoT, which is a LLMs based on llama2-13B PLM\n",
      "for finetuning and alignment learning. During the alignment training, we\n",
      "proposed the SimRRHF algorithm and Incorrect Data Relearning and mainly focused\n",
      "on optimizing the Chinese mathematical reasoning ability of Olapa-MCoT. The\n",
      "experiment achieved significant results, with the accuracy of Chinese\n",
      "mathematical reasoning up to 50%, 36% rise compared to llama2-13B. In addition,\n",
      "the accuracy of English reasoning ability also increased by nearly 4%.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17535v1\n",
      "summary: Olapa-MCoT: LLMsにおける中国語数学推論能力の向上\n",
      "\n",
      "- Olapa-MCoTはLLMsの中国語数学推論能力を最適化するために提案された。\n",
      "- 提案手法SimRRHFアルゴリズムとIncorrect Data Relearningにより、実験結果は顕著な成果を達成した。\n",
      "- Olapa-MCoTの中国語数学推論の精度は50%で、llama2-13Bと比較して36%向上し、さらに英語推論精度も約4%向上した。\n",
      "--------------------------------------------------\n",
      "title: Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception\n",
      "published: 2023-12-29 09:29:37+00:00\n",
      "abstruct: Quantities are distinct and critical components of texts that characterize\n",
      "the magnitude properties of entities, providing a precise perspective for the\n",
      "understanding of natural language, especially for reasoning tasks. In recent\n",
      "years, there has been a flurry of research on reasoning tasks based on large\n",
      "language models (LLMs), most of which solely focus on numerical values,\n",
      "neglecting the dimensional concept of quantities with units despite its\n",
      "importance. We argue that the concept of dimension is essential for precisely\n",
      "understanding quantities and of great significance for LLMs to perform\n",
      "quantitative reasoning. However, the lack of dimension knowledge and\n",
      "quantity-related benchmarks has resulted in low performance of LLMs. Hence, we\n",
      "present a framework to enhance the quantitative reasoning ability of language\n",
      "models based on dimension perception. We first construct a dimensional unit\n",
      "knowledge base (DimUnitKB) to address the knowledge gap in this area. We\n",
      "propose a benchmark DimEval consisting of seven tasks of three categories to\n",
      "probe and enhance the dimension perception skills of LLMs. To evaluate the\n",
      "effectiveness of our methods, we propose a quantitative reasoning task and\n",
      "conduct experiments. The experimental results show that our dimension\n",
      "perception method dramatically improves accuracy (43.55%->50.67%) on\n",
      "quantitative reasoning tasks compared to GPT-4.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17532v1\n",
      "summary: Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception\n",
      "\n",
      "- 数量の理解において次元の概念は重要であり、それを考慮することで大規模言語モデルの数量の推論能力を向上させることができる。\n",
      "- 欠如していた次元知識と数量に関するベンチマークの提案により、提案手法は大規模言語モデルの数量推論能力の向上に効果的であることを実験的に示した。\n",
      "- 次元知識ベース（DimUnitKB）の構築と、次元知覚スキルを向上させるための7つのタスクからなるDimEvalベンチマークの提案を行った。\n",
      "--------------------------------------------------\n",
      "title: Overview of the PromptCBLUE Shared Task in CHIP2023\n",
      "published: 2023-12-29 09:05:00+00:00\n",
      "abstruct: This paper presents an overview of the PromptCBLUE shared task\n",
      "(http://cips-chip.org.cn/2023/eval1) held in the CHIP-2023 Conference. This\n",
      "shared task reformualtes the CBLUE benchmark, and provide a good testbed for\n",
      "Chinese open-domain or medical-domain large language models (LLMs) in general\n",
      "medical natural language processing. Two different tracks are held: (a) prompt\n",
      "tuning track, investigating the multitask prompt tuning of LLMs, (b) probing\n",
      "the in-context learning capabilities of open-sourced LLMs. Many teams from both\n",
      "the industry and academia participated in the shared tasks, and the top teams\n",
      "achieved amazing test results. This paper describes the tasks, the datasets,\n",
      "evaluation metrics, and the top systems for both tasks. Finally, the paper\n",
      "summarizes the techniques and results of the evaluation of the various\n",
      "approaches explored by the participating teams.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17522v1\n",
      "summary: PromptCBLUE Shared Task in CHIP2023の概要\n",
      "\n",
      "- PromptCBLUE shared taskはCBLUEベンチマークを改変し、中国語のオープンドメインまたは医学ドメインの大規模言語モデル（LLMs）のテストベッドを提供する。\n",
      "- 2つのトラックがあり、prompt tuning trackではLLMsのマルチタスクプロンプトチューニングを調査し、もう1つのトラックではオープンソースのLLMsのコンテキスト内学習能力を調査する。\n",
      "- 産業界と学術界から多くのチームが参加し、トップチームは驚異的なテスト結果を達成した。\n",
      "--------------------------------------------------\n",
      "title: Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game\n",
      "published: 2023-12-29 08:26:54+00:00\n",
      "abstruct: Multi-agent collaboration with Large Language Models (LLMs) demonstrates\n",
      "proficiency in basic tasks, yet its efficiency in more complex scenarios\n",
      "remains unexplored. In gaming environments, these agents often face situations\n",
      "without established coordination protocols, requiring them to make intelligent\n",
      "inferences about teammates from limited data. This problem motivates the area\n",
      "of ad hoc teamwork, in which an agent may potentially cooperate with a variety\n",
      "of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork\n",
      "problem where the agent operates in an environment driven by natural language.\n",
      "Our findings reveal the potential of LLM agents in team collaboration,\n",
      "highlighting issues related to hallucinations in communication. To address this\n",
      "issue, we develop CodeAct, a general agent that equips LLM with enhanced memory\n",
      "and code-driven reasoning, enabling the repurposing of partial information for\n",
      "rapid adaptation to new teammates.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17515v1\n",
      "summary: 協力をつけて:アヴァロンゲームにおける即興チームワークのための言語エージェントの探索\n",
      "\n",
      "- 言語モデルを用いた多エージェント協力は基本的なタスクにおいて高い能力を示す\n",
      "- エージェント同士の即興チームワークにおいて言語による状況把握が重要\n",
      "- コード駆動の推論によるエージェントの記憶力と迅速な適応性が重要\n",
      "--------------------------------------------------\n",
      "title: Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning\n",
      "published: 2023-12-29 06:50:38+00:00\n",
      "abstruct: The surge in interest and application of large language models (LLMs) has\n",
      "sparked a drive to fine-tune these models to suit specific applications, such\n",
      "as finance and medical science. However, concerns regarding data privacy have\n",
      "emerged, especially when multiple stakeholders aim to collaboratively enhance\n",
      "LLMs using sensitive data. In this scenario, federated learning becomes a\n",
      "natural choice, allowing decentralized fine-tuning without exposing raw data to\n",
      "central servers. Motivated by this, we investigate how data privacy can be\n",
      "ensured in LLM fine-tuning through practical federated learning approaches,\n",
      "enabling secure contributions from multiple parties to enhance LLMs. Yet,\n",
      "challenges arise: 1) despite avoiding raw data exposure, there is a risk of\n",
      "inferring sensitive information from model outputs, and 2) federated learning\n",
      "for LLMs incurs notable communication overhead. To address these challenges,\n",
      "this article introduces DP-LoRA, a novel federated learning algorithm tailored\n",
      "for LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that\n",
      "adds noise in weight updates, maintaining individual data privacy while\n",
      "facilitating collaborative model training. Moreover, DP-LoRA optimizes\n",
      "communication efficiency via low-rank adaptation, minimizing the transmission\n",
      "of updated weights during distributed training. The experimental results across\n",
      "medical, financial, and general datasets using various LLMs demonstrate that\n",
      "DP-LoRA effectively ensures strict privacy constraints while minimizing\n",
      "communication overhead.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17493v1\n",
      "summary: 差分プライバシーを考慮したDP-LoRAアルゴリズム\n",
      "\n",
      "- 多数のステークホルダーが感じるLLMのプライバシー保護の重要性\n",
      "- DP-LoRAは重みの更新にノイズを加えることでデータプライバシーを保護\n",
      "- 通信オーバーヘッドを最小限に抑えながらプライバシー制約を満たすDP-LoRA\n",
      "--------------------------------------------------\n",
      "title: The Right Prompts for the Job: Repair Code-Review Defects with Large Language Model\n",
      "published: 2023-12-29 06:12:15+00:00\n",
      "abstruct: Automatic program repair (APR) techniques have the potential to reduce manual\n",
      "efforts in uncovering and repairing program defects during the code review (CR)\n",
      "process. However, the limited accuracy and considerable time costs associated\n",
      "with existing APR approaches hinder their adoption in industrial practice. One\n",
      "key factor is the under-utilization of review comments, which provide valuable\n",
      "insights into defects and potential fixes. Recent advancements in Large\n",
      "Language Models (LLMs) have enhanced their ability to comprehend natural and\n",
      "programming languages, enabling them to generate patches based on review\n",
      "comments. This paper conducts a comprehensive investigation into the effective\n",
      "utilization of LLMs for repairing CR defects. In this study, various prompts\n",
      "are designed and compared across mainstream LLMs using two distinct datasets\n",
      "from human reviewers and automated checkers. Experimental results demonstrate a\n",
      "remarkable repair rate of 72.97% with the best prompt, highlighting a\n",
      "substantial improvement in the effectiveness and practicality of automatic\n",
      "repair techniques.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17485v1\n",
      "summary: The Right Prompts for the Job: Repair Code-Review Defects with Large Language Model\n",
      "\n",
      "- 自動プログラム修復技術の有効な利用\n",
      "- 大規模言語モデルを使用した修復率の向上\n",
      "- レビューコメントを活用した修復プロンプトの重要性\n",
      "--------------------------------------------------\n",
      "title: Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning\n",
      "published: 2023-12-29 06:08:18+00:00\n",
      "abstruct: Despite the great success of large language models (LLMs) in various tasks,\n",
      "they suffer from generating hallucinations. We introduce Truth Forest, a method\n",
      "that enhances truthfulness in LLMs by uncovering hidden truth representations\n",
      "using multi-dimensional orthogonal probes. Specifically, it creates multiple\n",
      "orthogonal bases for modeling truth by incorporating orthogonal constraints\n",
      "into the probes. Moreover, we introduce Random Peek, a systematic technique\n",
      "considering an extended range of positions within the sequence, reducing the\n",
      "gap between discerning and generating truth features in LLMs. By employing this\n",
      "approach, we improved the truthfulness of Llama-2-7B from 40.8\\% to 74.5\\% on\n",
      "TruthfulQA. Likewise, significant improvements are observed in fine-tuned\n",
      "models. We conducted a thorough analysis of truth features using probes. Our\n",
      "visualization results show that orthogonal probes capture complementary\n",
      "truth-related features, forming well-defined clusters that reveal the inherent\n",
      "structure of the dataset. Code: \\url{https://github.com/jongjyh/trfr}\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17484v1\n",
      "summary: Truth Forest: 大規模言語モデルにおける多スケールな真実性に向けた介入を通じた真実性\n",
      "\n",
      "- 大規模言語モデル(LLMs)の真実性を向上させる手法「Truth Forest」を導入\n",
      "- 複数の直交プローブを使用し、真実性に関連する特徴をキャプチャし、データセットの固有構造を明らかにする\n",
      "- Random Peekを導入し、系列内の広範囲の位置を考慮して真実性の特徴を生成し、識別する差を縮小\n",
      "--------------------------------------------------\n",
      "title: Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters\n",
      "published: 2023-12-29 05:19:11+00:00\n",
      "abstruct: The advancement of Large Language Models (LLMs) has led to their widespread\n",
      "use across a broad spectrum of tasks including decision making. Prior studies\n",
      "have compared the decision making abilities of LLMs with those of humans from a\n",
      "psychological perspective. However, these studies have not always properly\n",
      "accounted for the sensitivity of LLMs' behavior to hyperparameters and\n",
      "variations in the prompt. In this study, we examine LLMs' performance on the\n",
      "Horizon decision making task studied by Binz and Schulz (2023) analyzing how\n",
      "LLMs respond to variations in prompts and hyperparameters. By experimenting on\n",
      "three OpenAI language models possessing different capabilities, we observe that\n",
      "the decision making abilities fluctuate based on the input prompts and\n",
      "temperature settings. Contrary to previous findings language models display a\n",
      "human-like exploration exploitation tradeoff after simple adjustments to the\n",
      "prompt.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17476v1\n",
      "summary: 大規模言語モデル（LLMs）の決定能力の感度について\n",
      "\n",
      "- LLMsの振る舞いは入力プロンプトや温度設定に基づいて変動する。\n",
      "- 先行研究の結果に反して、言語モデルはプロンプトの簡単な調整により、人間のような探索と活用のトレードオフを示す。\n",
      "- LLMsの決定能力は、モデルの特性によって異なる。\n",
      "--------------------------------------------------\n",
      "title: EHR Interaction Between Patients and AI: NoteAid EHR Interaction\n",
      "published: 2023-12-29 05:13:40+00:00\n",
      "abstruct: With the rapid advancement of Large Language Models (LLMs) and their\n",
      "outstanding performance in semantic and contextual comprehension, the potential\n",
      "of LLMs in specialized domains warrants exploration. This paper introduces the\n",
      "NoteAid EHR Interaction Pipeline, an innovative approach developed using\n",
      "generative LLMs to assist in patient education, a task stemming from the need\n",
      "to aid patients in understanding Electronic Health Records (EHRs). Building\n",
      "upon the NoteAid work, we designed two novel tasks from the patient's\n",
      "perspective: providing explanations for EHR content that patients may not\n",
      "understand and answering questions posed by patients after reading their EHRs.\n",
      "We extracted datasets containing 10,000 instances from MIMIC Discharge\n",
      "Summaries and 876 instances from the MADE medical notes collection,\n",
      "respectively, executing the two tasks through the NoteAid EHR Interaction\n",
      "Pipeline with these data. Performance data of LLMs on these tasks were\n",
      "collected and constructed as the corresponding NoteAid EHR Interaction Dataset.\n",
      "Through a comprehensive evaluation of the entire dataset using LLM assessment\n",
      "and a rigorous manual evaluation of 64 instances, we showcase the potential of\n",
      "LLMs in patient education. Besides, the results provide valuable data support\n",
      "for future exploration and applications in this domain while also supplying\n",
      "high-quality synthetic datasets for in-house system training.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17475v1\n",
      "summary: EHRとAIの相互作用: NoteAid EHR Interaction\n",
      "\n",
      "- 大規模言語モデル(LLM)を用いたNoteAid EHR Interaction Pipelineは、患者の健康記録を理解するための革新的なアプローチであり、患者教育を支援する可能性がある。\n",
      "- LLMの性能データを収集し、NoteAid EHR Interaction Datasetを構築。これにより、患者教育におけるLLMの潜在能力を示唆し、将来の探索や応用に有益なデータを提供。\n",
      "- MIMIC Discharge SummariesとMADE医療ノートコレクションからのデータを用いて、NoteAid EHR Interaction Pipelineを実行し、高品質の合成データセットを構築。\n",
      "--------------------------------------------------\n",
      "title: DB-GPT: Empowering Database Interactions with Private Large Language Models\n",
      "published: 2023-12-29 03:23:23+00:00\n",
      "abstruct: The recent breakthroughs in large language models (LLMs) are positioned to\n",
      "transition many areas of software. Database technologies particularly have an\n",
      "important entanglement with LLMs as efficient and intuitive database\n",
      "interactions are paramount. In this paper, we present DB-GPT, a revolutionary\n",
      "and production-ready project that integrates LLMs with traditional database\n",
      "systems to enhance user experience and accessibility. DB-GPT is designed to\n",
      "understand natural language queries, provide context-aware responses, and\n",
      "generate complex SQL queries with high accuracy, making it an indispensable\n",
      "tool for users ranging from novice to expert. The core innovation in DB-GPT\n",
      "lies in its private LLM technology, which is fine-tuned on domain-specific\n",
      "corpora to maintain user privacy and ensure data security while offering the\n",
      "benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which\n",
      "includes a novel retrieval augmented generation (RAG) knowledge system, an\n",
      "adaptive learning mechanism to continuously improve performance based on user\n",
      "feedback and a service-oriented multi-model framework (SMMF) with powerful\n",
      "data-driven agents. Our extensive experiments and user studies confirm that\n",
      "DB-GPT represents a paradigm shift in database interactions, offering a more\n",
      "natural, efficient, and secure way to engage with data repositories. The paper\n",
      "concludes with a discussion of the implications of DB-GPT framework on the\n",
      "future of human-database interaction and outlines potential avenues for further\n",
      "enhancements and applications in the field. The project code is available at\n",
      "https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by\n",
      "installing it with the instructions\n",
      "https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute\n",
      "video at https://www.youtube.com/watch?v=KYs4nTDzEhk.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17449v1\n",
      "summary: DB-GPT: プライベート大規模言語モデルを活用したデータベースインタラクションの強化\n",
      "\n",
      "- DB-GPTは自然言語クエリを理解し、コンテキストに適した応答を提供し、高い精度で複雑なSQLクエリを生成する。\n",
      "- プライベートLLM技術を使用し、ユーザーのプライバシーとデータセキュリティを維持しながら、最先端のLLMの利点を提供する。\n",
      "- DB-GPTはデータベースとのインタラクションにおいて、より自然で効率的、かつ安全な方法を提供し、パラダイムシフトを象徴する。\n",
      "--------------------------------------------------\n",
      "title: SMoT: Think in State Machine\n",
      "published: 2023-12-29 03:00:04+00:00\n",
      "abstruct: Current prompting approach for language model inference mainly rely on\n",
      "Language Model's (LLM) autonomous exploration of reasoning paths, confronts an\n",
      "inevitable retracing operation when erroneous routes are encountered. This is\n",
      "followed by the pursuit of alternative reasoning paths. However, humans are\n",
      "adept at abstracting optimal solutions from problems, thereby facilitating\n",
      "swift and precise reasoning for similar problems resolution. In light of this,\n",
      "we delves into the potential of harnessing expert knowledge to enhance\n",
      "problem-solving within LLMs. We introduce a novel paradigm, the State Machine\n",
      "of Thought (SMoT), which employs predefined state machines to furnish LLMs with\n",
      "efficient reasoning paths, thereby eliminating fruitless exploration.\n",
      "Furthermore, we propose a multi-agent mechanism that assigns different\n",
      "objectives to agents, aiming to enhance the accuracy of SMoT reasoning. The\n",
      "experimental results, derived from an array reasoning task, reveal that SMoT\n",
      "realizes an extraordinary accuracy of 95\\%, surpassing the performance of the\n",
      "state-of-the-art baselines.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17445v1\n",
      "summary: SMoT: Think in State Machine\n",
      "\n",
      "- 現在の言語モデル推論の問題点は、誤ったルートに遭遇した場合に再度探索を行う必要があり、効率が低い。\n",
      "- 人間は最適な解決策を抽象化し、同様の問題の解決に迅速かつ正確な推論を行う能力を持っている。\n",
      "- 新しいパラダイムであるSMoTは、事前定義された状態機械を使用して言語モデルに効率的な推論経路を提供し、無駄な探索を排除する。\n",
      "--------------------------------------------------\n",
      "title: Video Understanding with Large Language Models: A Survey\n",
      "published: 2023-12-29 01:56:17+00:00\n",
      "abstruct: With the burgeoning growth of online video platforms and the escalating\n",
      "volume of video content, the demand for proficient video understanding tools\n",
      "has intensified markedly. With Large Language Models (LLMs) showcasing\n",
      "remarkable capabilities in key language tasks, this survey provides a detailed\n",
      "overview of the recent advancements in video understanding harnessing the power\n",
      "of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\n",
      "advanced, particularly their ability for open-ended spatial-temporal reasoning\n",
      "combined with commonsense knowledge, suggesting a promising path for future\n",
      "video understanding. We examine the unique characteristics and capabilities of\n",
      "Vid-LLMs, categorizing the approaches into four main types: LLM-based Video\n",
      "Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.\n",
      "Furthermore, this survey also presents a comprehensive study of the tasks and\n",
      "datasets for Vid-LLMs, along with the methodologies employed for evaluation.\n",
      "Additionally, the survey explores the expansive applications of Vid-LLMs across\n",
      "various domains, thereby showcasing their remarkable scalability and\n",
      "versatility in addressing challenges in real-world video understanding.\n",
      "Finally, the survey summarizes the limitations of existing Vid-LLMs and the\n",
      "directions for future research. For more information, we recommend readers\n",
      "visit the repository at\n",
      "https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17432v1\n",
      "summary: 動画理解における大規模言語モデル: 調査\n",
      "\n",
      "- オープンエンドの時空間推論と常識的知識を組み合わせたVid-LLMsの能力が注目されている\n",
      "- Vid-LLMsの4つの主要なアプローチ：LLMベースのビデオエージェント、Vid-LLMsの事前学習、Vid-LLMsの指示チューニング、ハイブリッド手法\n",
      "- Vid-LLMsの幅広い応用領域と、実世界の動画理解の課題に対する驚異的な拡張性と柔軟性\n",
      "--------------------------------------------------\n",
      "title: Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach\n",
      "published: 2023-12-28 20:41:24+00:00\n",
      "abstruct: This paper introduces Auto-modeling of Formal Verification with Real-world\n",
      "Prompting for 5G and NextG protocols (AVRE), a novel system designed for the\n",
      "formal verification of Next Generation (NextG) communication protocols,\n",
      "addressing the increasing complexity and scalability challenges in network\n",
      "protocol design and verification. Utilizing Large Language Models (LLMs), AVRE\n",
      "transforms protocol descriptions into dependency graphs and formal models,\n",
      "efficiently resolving ambiguities and capturing design intent. The system\n",
      "integrates a transformer model with LLMs to autonomously establish quantifiable\n",
      "dependency relationships through cross- and self-attention mechanisms. Enhanced\n",
      "by iterative feedback from the HyFuzz experimental platform, AVRE significantly\n",
      "advances the accuracy and relevance of formal verification in complex\n",
      "communication protocols, offering a groundbreaking approach to validating\n",
      "sophisticated communication systems. We compare CAL's performance with\n",
      "state-of-the-art LLM-based models and traditional time sequence models,\n",
      "demonstrating its superiority in accuracy and robustness, achieving an accuracy\n",
      "of 95.94\\% and an AUC of 0.98. This NLP-based approach enables, for the first\n",
      "time, the creation of exploits directly from design documents, making\n",
      "remarkable progress in scalable system verification and validation.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17353v1\n",
      "summary: Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach\n",
      "\n",
      "- AVREは、次世代通信プロトコルの形式検証のための革新的なシステムであり、大規模言語モデル（LLM）を活用してプロトコルの記述を依存グラフや形式モデルに変換する。\n",
      "- AVREはtransformerモデルとLLMを統合し、クロスアテンションとセルフアテンションのメカニズムを介して独自に定量的な依存関係を確立する。\n",
      "- AVREは、HyFuzz実験プラットフォームからの反復的なフィードバックによって強化され、複雑な通信プロトコルの形式検証の精度と関連性を大幅に向上させる。\n",
      "--------------------------------------------------\n",
      "title: AQUALLM: Audio Question Answering Data Generation Using Large Language Models\n",
      "published: 2023-12-28 20:01:27+00:00\n",
      "abstruct: Audio Question Answering (AQA) constitutes a pivotal task in which machines\n",
      "analyze both audio signals and natural language questions to produce precise\n",
      "natural language answers. The significance of possessing high-quality, diverse,\n",
      "and extensive AQA datasets cannot be overstated when aiming for the precision\n",
      "of an AQA system. While there has been notable focus on developing accurate and\n",
      "efficient AQA models, the creation of high-quality, diverse, and extensive\n",
      "datasets for the specific task at hand has not garnered considerable attention.\n",
      "To address this challenge, this work makes several contributions. We introduce\n",
      "a scalable AQA data generation pipeline, denoted as the AQUALLM framework,\n",
      "which relies on Large Language Models (LLMs). This framework utilizes existing\n",
      "audio-caption annotations and incorporates state-of-the-art LLMs to generate\n",
      "expansive, high-quality AQA datasets. Additionally, we present three extensive\n",
      "and high-quality benchmark datasets for AQA, contributing significantly to the\n",
      "progression of AQA research. AQA models trained on the proposed datasets set\n",
      "superior benchmarks compared to the existing state-of-the-art. Moreover, models\n",
      "trained on our datasets demonstrate enhanced generalizability when compared to\n",
      "models trained using human-annotated AQA data. Code and datasets will be\n",
      "accessible on GitHub~\\footnote{\\url{https://github.com/swarupbehera/AQUALLM}}.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17343v1\n",
      "summary: AQUALLM: 大規模言語モデルを使用した音声質問応データ生成\n",
      "\n",
      "- 高品質で多様なAQAデータセットの生成に成功\n",
      "- AQUALLMフレームワークは大規模言語モデルを活用\n",
      "- 提案されたデータセットで訓練されたモデルが既存の最先端技術を上回る高いベンチマークを設定\n",
      "--------------------------------------------------\n",
      "title: Learning Vision from Models Rivals Learning Vision from Data\n",
      "published: 2023-12-28 18:59:55+00:00\n",
      "abstruct: We introduce SynCLR, a novel approach for learning visual representations\n",
      "exclusively from synthetic images and synthetic captions, without any real\n",
      "data. We synthesize a large dataset of image captions using LLMs, then use an\n",
      "off-the-shelf text-to-image model to generate multiple images corresponding to\n",
      "each synthetic caption. We perform visual representation learning on these\n",
      "synthetic images via contrastive learning, treating images sharing the same\n",
      "caption as positive pairs. The resulting representations transfer well to many\n",
      "downstream tasks, competing favorably with other general-purpose visual\n",
      "representation learners such as CLIP and DINO v2 in image classification tasks.\n",
      "Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR\n",
      "outperforms previous self-supervised methods by a significant margin, e.g.,\n",
      "improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17742v1\n",
      "summary: Learning Vision from Models Rivals Learning Vision from Data\n",
      "\n",
      "- SynCLRは合成画像と合成キャプションだけから視覚表現を学習する革新的な手法。\n",
      "- 合成画像を対象にコントラスティブラーニングを行い、その結果得られた表現は多くのタスクにおいて競合手法に匹敵する性能を示す。\n",
      "- SynCLRはセマンティックセグメンテーションなどの密な予測タスクにおいても、従来の自己教師付き学習手法を大幅に上回る。\n",
      "--------------------------------------------------\n",
      "title: The LLM Surgeon\n",
      "published: 2023-12-28 18:59:09+00:00\n",
      "abstruct: State-of-the-art language models are becoming increasingly large in an effort\n",
      "to achieve the highest performance on large corpora of available textual data.\n",
      "However, the sheer size of the Transformer architectures makes it difficult to\n",
      "deploy models within computational, environmental or device-specific\n",
      "constraints. We explore data-driven compression of existing pretrained models\n",
      "as an alternative to training smaller models from scratch. To do so, we scale\n",
      "Kronecker-factored curvature approximations of the target loss landscape to\n",
      "large language models. In doing so, we can compute both the dynamic allocation\n",
      "of structures that can be removed as well as updates of remaining weights that\n",
      "account for the removal. We provide a general framework for unstructured,\n",
      "semi-structured and structured pruning and improve upon weight updates to\n",
      "capture more correlations between weights, while remaining computationally\n",
      "efficient. Experimentally, our method can prune rows and columns from a range\n",
      "of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance,\n",
      "and achieve state-of-the-art results in unstructured and semi-structured\n",
      "pruning of large language models.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17244v1\n",
      "summary: The LLM Surgeon\n",
      "\n",
      "- 最新の言語モデルは巨大で、そのサイズのためにモデルの展開が困難。\n",
      "- 既存の事前学習済みモデルのデータ駆動型の圧縮を探求し、小さなモデルをゼロから訓練する代替手段を提供。\n",
      "- 提案手法は、20%〜30%の剪定を可能にし、性能のほとんど損失なしに、大規模な言語モデルの非構造化および半構造化の剪定で最先端の結果を実現。\n",
      "--------------------------------------------------\n",
      "title: Fast Inference of Mixture-of-Experts Language Models with Offloading\n",
      "published: 2023-12-28 18:58:13+00:00\n",
      "abstruct: With the widespread adoption of Large Language Models (LLMs), many deep\n",
      "learning practitioners are looking for strategies of running these models more\n",
      "efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - a\n",
      "type of model architectures where only a fraction of model layers are active\n",
      "for any given input. This property allows MoE-based language models to generate\n",
      "tokens faster than their dense counterparts, but it also increases model size\n",
      "due to having multiple experts. Unfortunately, this makes state-of-the-art MoE\n",
      "language models difficult to run without high-end GPUs. In this work, we study\n",
      "the problem of running large MoE language models on consumer hardware with\n",
      "limited accelerator memory. We build upon parameter offloading algorithms and\n",
      "propose a novel strategy that accelerates offloading by taking advantage of\n",
      "innate properties of MoE LLMs. Using this strategy, we build can run\n",
      "Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google\n",
      "Colab instances.\n",
      "PDFリンク: http://arxiv.org/pdf/2312.17238v1\n",
      "summary: 高速推論を実現するMoE（Mixture-of-Experts）言語モデルの重要なポイント\n",
      "\n",
      "- MoEモデルはスパース性を利用してトークンを高速に生成できる\n",
      "- MoEモデルの大きさが増加するため、高性能なGPUがないと実行が難しい\n",
      "- 提案されたパラメータオフロードアルゴリズムにより、デスクトップハードウェアや無料のGoogle Colabインスタンスで大規模なMoE言語モデルを実行できる\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CATEGORIES = {\n",
    "#     \"cs.AI\",  # 例: コンピュータサイエンスのAI分野\n",
    "#     # \"stat.ML\",  # 例: 統計学の機械学習分野\n",
    "#     # 必要に応じて他のカテゴリーを追加\n",
    "# }\n",
    "\n",
    "keyword = \"LLM\"\n",
    "results = search_arxiv(keyword)\n",
    "\n",
    "for result in results:\n",
    "    summary = get_summary(result)\n",
    "    print(f\"title: {result.title}\")\n",
    "    print(f\"published: {result.published}\")\n",
    "    print(f\"abstruct: {result.summary}\")\n",
    "    print(f\"PDFリンク: {result.pdf_url}\")\n",
    "    print(f\"summary: {summary}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831de98-d671-4c61-a8c8-00dc641952ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
